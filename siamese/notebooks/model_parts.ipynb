{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of this as a scratchpad for writing our model.\n",
    "\n",
    "We will check our model by training it and printing loss for 100 steps. In next notebook, we will see a complete model with validation done after some interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from collections import namedtuple\n",
    "\n",
    "#Set this to actual path of the folder where your training, validation and vocab files reside\n",
    "DATA_DIR = 'ubuntu-data'\n",
    "\n",
    "#Training files: context, response and flag\n",
    "train_context_file = os.path.join(DATA_DIR, 'train.context')\n",
    "train_response_file = os.path.join(DATA_DIR, 'train.response')\n",
    "train_flag_file = os.path.join(DATA_DIR, 'train.flag')\n",
    "\n",
    "#Validation files: context, response and flag\n",
    "valid_context_file = os.path.join(DATA_DIR, 'valid.context')\n",
    "valid_response_file = os.path.join(DATA_DIR, 'valid.response')\n",
    "valid_flag_file = os.path.join(DATA_DIR, 'valid.flag')\n",
    "\n",
    "#Vocab file\n",
    "vocab_file = os.path.join(DATA_DIR, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should not seem new. We saw this in [dataset_ops notebook](https://github.com/vineetm/tensorflow-notes/blob/master/siamese/notebooks/dataset_ops.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice a new (sixth) field init. This is initializer for iterator\n",
    "class DataIterator(namedtuple('DataIterator', 'init context len_context response len_response flag')):\n",
    "    pass\n",
    "\n",
    "def text_to_word_indexes(text_file, vocab_table):\n",
    "    dataset = tf.data.TextLineDataset(text_file)\n",
    "    \n",
    "    #Split sentence to words\n",
    "    dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "    #Convert words to indexes\n",
    "    dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def create_dataset_iterator(vocab_table, context_file, response_file, flag_file, batch_size):\n",
    "    #Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "    vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "    #Create context dataset, sentence -> word indexes\n",
    "    context_dataset = text_to_word_indexes(context_file, vocab_table)\n",
    "\n",
    "    #Restrict context to Last 160 tokens\n",
    "    context_dataset = context_dataset.map(lambda words: words[-160:])\n",
    "\n",
    "    #Create response dataset, sentence -> word indexes\n",
    "    response_dataset = text_to_word_indexes(response_file, vocab_table)\n",
    "\n",
    "    flag_dataset = tf.data.TextLineDataset(flag_file)\n",
    "    # Convert string to a float..\n",
    "    flag_dataset = flag_dataset.map(lambda sentence: tf.string_to_number(sentence))\n",
    "\n",
    "    #Join datasets together, using zip\n",
    "    dataset = tf.data.Dataset.zip((context_dataset, response_dataset, flag_dataset))\n",
    "\n",
    "    #Add length of context and response\n",
    "    dataset = dataset.map(lambda context, response, flag: (context, tf.size(context), response, tf.size(response), flag))\n",
    "\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([])))\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    context, len_context, response, len_response, flag = iterator.get_next()\n",
    "\n",
    "    return DataIterator(iterator.initializer, context, len_context, response, len_response, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "iterator = create_dataset_iterator(vocab_table, train_context_file, train_response_file, train_flag_file, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is size of `vocab.txt`\n",
    "V = 30430\n",
    "d = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a word embeddings Tensor. Think of it as index -> vector\n",
    "\n",
    "Next, we convert word indexes in *context* to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embeddings (this converts index -> vector)\n",
    "W = tf.get_variable(name='word_embeddings', shape=[V, d])\n",
    "\n",
    "# Gathers all words together: Shape: batch_size x T x d\n",
    "context = tf.nn.embedding_lookup(W, iterator.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we convert context word vectors to a single fixed length vector using RNN.\n",
    "\n",
    "* We first define the RNN Cell. We are using a LSTM cell\n",
    "* Next we use dynamic_rnn. We are ignoring intermediate outputs, and are only interested in final state\n",
    "* Final state thus, helps us create `c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cell = tf.contrib.rnn.BasicLSTMCell(d)\n",
    "\n",
    "with tf.variable_scope('rnn'):\n",
    "    _, state_context = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=context, \n",
    "                                         sequence_length=iterator.len_context, dtype=tf.float32)\n",
    "c = state_context.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a similar operation to context to get `r`\n",
    "\n",
    "* Here, we are re-using RNN weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathers all words together: Shape: batch_size x T x d\n",
    "response = tf.nn.embedding_lookup(W, iterator.response)\n",
    "with tf.variable_scope('rnn', reuse=True):\n",
    "    _, state_response = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=response, \n",
    "                                         sequence_length=iterator.len_response, dtype=tf.float32)\n",
    "r = state_response.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, we define correlation weights `M`. Based on recommendation in the [original paper](www.cs.toronto.edu/~lcharlin/papers/ubuntu_dialogue_dd17.pdf) we set this as an identity matrix! \n",
    "* We further compute logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = tf.Variable(tf.eye(d), name='M')\n",
    "logits = tf.reduce_sum(tf.multiply(c, tf.matmul(r, M)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute loss with respect to `flag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=iterator.flag, logits=logits)\n",
    "loss = tf.reduce_mean(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: [<tf.Variable 'word_embeddings:0' shape=(30430, 128) dtype=float32_ref>, <tf.Variable 'rnn/rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>, <tf.Variable 'rnn/rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'M:0' shape=(128, 128) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "opt = tf.train.AdamOptimizer(0.001)\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "print('Trainable params: %s'%params)\n",
    "gradients = tf.gradients(loss, params)\n",
    "clipped_gradients, grad_norm = tf.clip_by_global_norm(gradients, 5.0)\n",
    "train_step = opt.apply_gradients(zip(clipped_gradients, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:0 Loss:0.6931110620498657\n",
      "Step:1 Loss:0.693158745765686\n",
      "Step:2 Loss:0.6929095387458801\n",
      "Step:3 Loss:0.6928654909133911\n",
      "Step:4 Loss:0.692034900188446\n",
      "Step:5 Loss:0.6983200311660767\n",
      "Step:6 Loss:0.6959814429283142\n",
      "Step:7 Loss:0.6930999755859375\n",
      "Step:8 Loss:0.6922240257263184\n",
      "Step:9 Loss:0.6931871771812439\n",
      "Step:10 Loss:0.692741870880127\n",
      "Step:11 Loss:0.6931243538856506\n",
      "Step:12 Loss:0.6931833028793335\n",
      "Step:13 Loss:0.6928664445877075\n",
      "Step:14 Loss:0.6929497718811035\n",
      "Step:15 Loss:0.6936433911323547\n",
      "Step:16 Loss:0.6932743191719055\n",
      "Step:17 Loss:0.6933149099349976\n",
      "Step:18 Loss:0.692735493183136\n",
      "Step:19 Loss:0.6932310461997986\n",
      "Step:20 Loss:0.6932319402694702\n",
      "Step:21 Loss:0.6925898790359497\n",
      "Step:22 Loss:0.6929774284362793\n",
      "Step:23 Loss:0.6920992136001587\n",
      "Step:24 Loss:0.6928004026412964\n",
      "Step:25 Loss:0.6943953037261963\n",
      "Step:26 Loss:0.694351315498352\n",
      "Step:27 Loss:0.6948292851448059\n",
      "Step:28 Loss:0.69340580701828\n",
      "Step:29 Loss:0.6929185390472412\n",
      "Step:30 Loss:0.6934899091720581\n",
      "Step:31 Loss:0.6926593780517578\n",
      "Step:32 Loss:0.6930112838745117\n",
      "Step:33 Loss:0.6933994293212891\n",
      "Step:34 Loss:0.6931432485580444\n",
      "Step:35 Loss:0.6933096647262573\n",
      "Step:36 Loss:0.693177342414856\n",
      "Step:37 Loss:0.6933332085609436\n",
      "Step:38 Loss:0.6930949687957764\n",
      "Step:39 Loss:0.6931115388870239\n",
      "Step:40 Loss:0.6932401657104492\n",
      "Step:41 Loss:0.693067193031311\n",
      "Step:42 Loss:0.6930731534957886\n",
      "Step:43 Loss:0.693161129951477\n",
      "Step:44 Loss:0.6930410265922546\n",
      "Step:45 Loss:0.6931467056274414\n",
      "Step:46 Loss:0.693282961845398\n",
      "Step:47 Loss:0.6932485103607178\n",
      "Step:48 Loss:0.6931461691856384\n",
      "Step:49 Loss:0.6930174231529236\n",
      "Step:50 Loss:0.693078875541687\n",
      "Step:51 Loss:0.6931750774383545\n",
      "Step:52 Loss:0.6930997371673584\n",
      "Step:53 Loss:0.6931803822517395\n",
      "Step:54 Loss:0.6931591033935547\n",
      "Step:55 Loss:0.693095326423645\n",
      "Step:56 Loss:0.6932280659675598\n",
      "Step:57 Loss:0.6931328773498535\n",
      "Step:58 Loss:0.6930931210517883\n",
      "Step:59 Loss:0.6931449174880981\n",
      "Step:60 Loss:0.6930950880050659\n",
      "Step:61 Loss:0.6931584477424622\n",
      "Step:62 Loss:0.693114161491394\n",
      "Step:63 Loss:0.6931862831115723\n",
      "Step:64 Loss:0.6931867003440857\n",
      "Step:65 Loss:0.6927536725997925\n",
      "Step:66 Loss:0.6929100751876831\n",
      "Step:67 Loss:0.6930553317070007\n",
      "Step:68 Loss:0.6934428215026855\n",
      "Step:69 Loss:0.6932954788208008\n",
      "Step:70 Loss:0.6931691765785217\n",
      "Step:71 Loss:0.6934518814086914\n",
      "Step:72 Loss:0.6930232644081116\n",
      "Step:73 Loss:0.6931145191192627\n",
      "Step:74 Loss:0.6927698254585266\n",
      "Step:75 Loss:0.6924701929092407\n",
      "Step:76 Loss:0.692786455154419\n",
      "Step:77 Loss:0.6924580335617065\n",
      "Step:78 Loss:0.6922591924667358\n",
      "Step:79 Loss:0.6921626329421997\n",
      "Step:80 Loss:0.6966894268989563\n",
      "Step:81 Loss:0.6919490694999695\n",
      "Step:82 Loss:0.6919496059417725\n",
      "Step:83 Loss:0.6902545690536499\n",
      "Step:84 Loss:0.6900839805603027\n",
      "Step:85 Loss:0.6883882284164429\n",
      "Step:86 Loss:0.6820123791694641\n",
      "Step:87 Loss:0.7014718651771545\n",
      "Step:88 Loss:0.6786831617355347\n",
      "Step:89 Loss:0.7046576738357544\n",
      "Step:90 Loss:0.6993597745895386\n",
      "Step:91 Loss:0.6349215507507324\n",
      "Step:92 Loss:0.668381929397583\n",
      "Step:93 Loss:0.6191630363464355\n",
      "Step:94 Loss:0.6883033514022827\n",
      "Step:95 Loss:0.7007017731666565\n",
      "Step:96 Loss:0.5920140743255615\n",
      "Step:97 Loss:0.9747644662857056\n",
      "Step:98 Loss:0.6054667830467224\n",
      "Step:99 Loss:0.6536471247673035\n"
     ]
    }
   ],
   "source": [
    "#Let us run our model for 100 steps\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.init)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100):\n",
    "        loss_val, _ = sess.run([loss, train_step])\n",
    "        print('Step:{} Loss:{}'.format(step, loss_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
