{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first define all relevant file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "from collections import namedtuple\n",
    "\n",
    "#Set this to actual path of the folder where your training, validation and vocab files reside\n",
    "DATA_DIR = 'ubuntu-data'\n",
    "\n",
    "#Training files: context, response and flag\n",
    "train_context_file = os.path.join(DATA_DIR, 'train.context')\n",
    "train_response_file = os.path.join(DATA_DIR, 'train.response')\n",
    "train_flag_file = os.path.join(DATA_DIR, 'train.flag')\n",
    "\n",
    "#Validation files: context, response and flag\n",
    "valid_context_file = os.path.join(DATA_DIR, 'valid.context')\n",
    "valid_response_file = os.path.join(DATA_DIR, 'valid.response')\n",
    "valid_flag_file = os.path.join(DATA_DIR, 'valid.flag')\n",
    "\n",
    "#Vocab file\n",
    "vocab_file = os.path.join(DATA_DIR, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should not seem new. We saw this in [dataset_ops notebook](https://github.com/vineetm/tensorflow-notes/blob/master/siamese/notebooks/dataset_ops.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice a new (sixth) field init. This is initializer for iterator\n",
    "class DataIterator(namedtuple('DataIterator', 'init context len_context response len_response flag')):\n",
    "    pass\n",
    "\n",
    "def text_to_word_indexes(text_file, vocab_table):\n",
    "    dataset = tf.data.TextLineDataset(text_file)\n",
    "    \n",
    "    #Split sentence to words\n",
    "    dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "    #Convert words to indexes\n",
    "    dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def create_dataset_iterator(vocab_table, context_file, response_file, flag_file, batch_size):\n",
    "    #Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "    vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "    #Create context dataset, sentence -> word indexes\n",
    "    context_dataset = text_to_word_indexes(context_file, vocab_table)\n",
    "\n",
    "    #Restrict context to Last 160 tokens\n",
    "    context_dataset = context_dataset.map(lambda words: words[-160:])\n",
    "\n",
    "    #Create response dataset, sentence -> word indexes\n",
    "    response_dataset = text_to_word_indexes(response_file, vocab_table)\n",
    "\n",
    "    flag_dataset = tf.data.TextLineDataset(flag_file)\n",
    "    # Convert string to a float..\n",
    "    flag_dataset = flag_dataset.map(lambda sentence: tf.string_to_number(sentence))\n",
    "\n",
    "    #Join datasets together, using zip\n",
    "    dataset = tf.data.Dataset.zip((context_dataset, response_dataset, flag_dataset))\n",
    "\n",
    "    #Add length of context and response\n",
    "    dataset = dataset.map(lambda context, response, flag: (context, tf.size(context), response, tf.size(response), flag))\n",
    "\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([])))\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    context, len_context, response, len_response, flag = iterator.get_next()\n",
    "\n",
    "    return DataIterator(iterator.initializer, context, len_context, response, len_response, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn import ModeKeys\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, V, d, iterator, mode, lr=0.001):\n",
    "        #Mode is either Train or evaluation!\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.iterator = iterator\n",
    "        \n",
    "        #Common parts to train and evaluation\n",
    "        self.W = tf.get_variable(name='word_embeddings', shape=[V, d])\n",
    "        \n",
    "        #Context -> c\n",
    "        self.context = tf.nn.embedding_lookup(self.W, self.iterator.context)\n",
    "        rnn_cell = tf.contrib.rnn.BasicLSTMCell(d)\n",
    "        with tf.variable_scope('rnn'):\n",
    "            _, state_context = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=self.context, \n",
    "                                                 sequence_length=self.iterator.len_context, dtype=tf.float32)\n",
    "        c = state_context.h\n",
    "        \n",
    "        #Response -> c\n",
    "        response = tf.nn.embedding_lookup(self.W, self.iterator.response)\n",
    "        with tf.variable_scope('rnn', reuse=True):\n",
    "            _, state_response = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=response,\n",
    "                                                  sequence_length=self.iterator.len_response, dtype=tf.float32)\n",
    "        r = state_response.h\n",
    "\n",
    "        self.M = tf.Variable(tf.eye(d), name='M')\n",
    "        \n",
    "        #For checkpoint\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        \n",
    "        self.logits = tf.reduce_sum(tf.multiply(c, tf.matmul(r, self.M)), axis=1)\n",
    "        self.batch_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.iterator.flag, logits=self.logits)\n",
    "        self.loss = tf.reduce_mean(self.batch_loss)\n",
    "        \n",
    "        if self.mode == ModeKeys.TRAIN:\n",
    "            opt = tf.train.AdamOptimizer(lr)\n",
    "            params = tf.trainable_variables()\n",
    "            print('Trainable params: %s'%params)\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, grad_norm = tf.clip_by_global_norm(gradients, 5.0)\n",
    "            self.train_step = opt.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "    def eval(self, sess):\n",
    "        assert self.mode == ModeKeys.EVAL\n",
    "\n",
    "        #Initialize iterator\n",
    "        sess.run(self.iterator.init)\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        while True:\n",
    "          try:\n",
    "            total_loss += sess.run(self.loss)\n",
    "            num_batches += 1\n",
    "          except tf.errors.OutOfRangeError:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is size of `vocab.txt`\n",
    "V = 30430\n",
    "d = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: [<tf.Variable 'word_embeddings:0' shape=(30430, 128) dtype=float32_ref>, <tf.Variable 'rnn/rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>, <tf.Variable 'rnn/rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'M:0' shape=(128, 128) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "    train_iterator = create_dataset_iterator(vocab_table, train_context_file, train_response_file, \n",
    "                                             train_flag_file, batch_size=16)\n",
    "    train_model = Model(V, d, train_iterator, ModeKeys.TRAIN)\n",
    "\n",
    "    train_sess = tf.Session()\n",
    "    train_sess.run(tf.global_variables_initializer())\n",
    "    train_sess.run(tf.tables_initializer())\n",
    "    train_sess.run(train_iterator.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_graph = tf.Graph()\n",
    "with valid_graph.as_default():\n",
    "    vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "    valid_iterator = create_dataset_iterator(vocab_table, valid_context_file, valid_response_file, \n",
    "                                             valid_flag_file, batch_size=16)\n",
    "    valid_model = Model(V, d, valid_iterator, ModeKeys.EVAL)\n",
    "\n",
    "    valid_sess = tf.Session()\n",
    "    valid_sess.run(tf.global_variables_initializer())\n",
    "    valid_sess.run(tf.tables_initializer())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define checkpoint directory\n",
    "CKPT_DIR = 'saved_model/'\n",
    "if not tf.gfile.Exists(CKPT_DIR):\n",
    "    print('Creating {}'.format(CKPT_DIR))\n",
    "    tf.gfile.MkDir(CKPT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Train_Loss: 0.6931326389312744\n",
      "INFO:tensorflow:Restoring parameters from saved_model/-0\n"
     ]
    }
   ],
   "source": [
    "for step in itertools.count():\n",
    "    _, loss = train_sess.run([train_model.train_step, train_model.loss])\n",
    "    if step % 10 == 0:\n",
    "        print('Step: {} Train_Loss: {}'.format(step, loss))\n",
    "        train_model.saver.save(train_sess, CKPT_DIR, step)\n",
    "        \n",
    "        latest_ckpt = tf.train.latest_checkpoint(CKPT_DIR)\n",
    "        valid_model.saver.restore(valid_sess, latest_ckpt)\n",
    "        val_loss = valid_model.eval(valid_sess)\n",
    "        print('Step: {} Valid_Loss: {}'.format(step, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
