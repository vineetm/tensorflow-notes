{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will build a dataset iterator.\n",
    "\n",
    "Our dataset iterator would return these fields:\n",
    "* init: Iterator initializer. We will see its use when we write the main model\n",
    "* context: context shape: `batch_size x max_len` (will not exceed 160, as we restricted our context length)\n",
    "* len_context: `batch_size x 1` Length of context for each element in batch\n",
    "* response: `batch_size x max_len`\n",
    "* len_response: `batch_size x 1`\n",
    "* flag: `batch_size x 1`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "I have learnt how to use dataset APIs, dynamic RNN and namedtuple from the excellent [tutorial](https://github.com/tensorflow/nmt) and [code](https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py) on NMT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first define all relevant file paths. \n",
    "\n",
    "This should not seem new. We created these files in [data_prep notebook](https://github.com/vineetm/tensorflow-notes/blob/master/siamese/data_prep.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "\n",
    "#Set this to actual path of the folder where your training, validation and vocab files reside\n",
    "DATA_DIR = 'ubuntu-data'\n",
    "\n",
    "#Training files: context, response and flag\n",
    "train_context_file = os.path.join(DATA_DIR, 'train.context')\n",
    "train_response_file = os.path.join(DATA_DIR, 'train.response')\n",
    "train_flag_file = os.path.join(DATA_DIR, 'train.flag')\n",
    "\n",
    "#Validation files: context, response and flag\n",
    "valid_context_file = os.path.join(DATA_DIR, 'valid.context')\n",
    "valid_response_file = os.path.join(DATA_DIR, 'valid.response')\n",
    "valid_flag_file = os.path.join(DATA_DIR, 'valid.flag')\n",
    "\n",
    "#Vocab file\n",
    "vocab_file = os.path.join(DATA_DIR, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now lets us quickly test this API. We will create an iterator that returns words indexes for a sentence in `train.context` using vocab file. \n",
    "\n",
    "If this seems hurried, I would encourage you to refer my [blog post](https://vineetm.github.io/tensorflow-textdata/) and [notebook](https://github.com/vineetm/tensorflow-notes/blob/master/dataset/notebooks/tf-text-data.ipynb) which explains the dataset API in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "#Context dataset, one line per sentence\n",
    "context_dataset = tf.data.TextLineDataset(train_context_file)\n",
    "\n",
    "#Split sentence to words\n",
    "context_dataset = context_dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "#Convert words to indexes\n",
    "context_dataset = context_dataset.map(lambda words: vocab_table.lookup(words))\n",
    "\n",
    "#Create iterator\n",
    "iterator = context_dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test our iterator. We would need to do that inside a *session*. Also, we would need to\n",
    "* initialize the iterator\n",
    "* initialize vocab tables `tf.tables_initializer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    3    96   266   105  2089     4   307  2708   338  1505     5    24\n",
      "    48    35   266    61     6   141   338  1153     8     3    96     9\n",
      "    12   727   197  4965     4  1390    19   597   431    13   197  2089\n",
      "  2118   115    13    35     1     2     9    81    38   230   416     6\n",
      "   414    10  3615  2923    20     0     1   444   134     1     2    93\n",
      "     1   567   266    21  3458    56  5523     0     1     2   152     7\n",
      "     1     2   108     1     2    27   344     1     9    12  5480   119\n",
      "    49 18947     7     1     2    80     1    24     9    30   159    27\n",
      "     4   109   142   438     1    28    10   861  2615  9046    15   363\n",
      "  1642     1     2    11    67    38  1506   155    49  2615  1116     7\n",
      "     1  1775    10   402  1065   102  2952    23     6    16    32     4\n",
      " 16527    45    27    16    23    11  1411     1     2    31  4935    12\n",
      "     0     7     1     2    27  3455  4935     5    24    10  1814 12791\n",
      "     1     2     3  1513    14   266    79   656     4     0     1     2\n",
      "    93     1     2    81    11  1733    10   124  2274    10  1142     7\n",
      "     1     2  2834     7     1     2  9047     0    95    20     0     8\n",
      "    65  1891    18     4  1247    20     4  1468    13    14    30     9\n",
      "     1     2  9047     4   300   124    11   140     7     1     2    80\n",
      "     8   137   934   206     8     1     2     3    81  1102     6  1624\n",
      "     9    75    42  2546     8    42    12   114    14    97   935   155\n",
      "     0  2209     5     3   947     7     1     2]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    data = sess.run(iterator.get_next())\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we would need to do the same operation for `context` and `response`. Let us write a function, that would take a text file and vocab_table and return a dataset with words converted to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_indexes(text_file, vocab_table):\n",
    "    dataset = tf.data.TextLineDataset(text_file)\n",
    "    \n",
    "    #Split sentence to words\n",
    "    dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "    #Convert words to indexes\n",
    "    dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what do we do about flag? Flag needs to be converted to a float, use `tf.string_to_number`\n",
    "\n",
    "Finally, we need to return context, response and flag in a single iterator call. Enter, [`tf.data.Dataset.zip`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip)\n",
    "\n",
    "Let us see how this all works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "#Create context dataset, sentence -> word indexes\n",
    "context_dataset = text_to_word_indexes(train_context_file, vocab_table)\n",
    "\n",
    "#Create response dataset, sentence -> word indexes\n",
    "response_dataset = text_to_word_indexes(train_response_file, vocab_table)\n",
    "\n",
    "flag_dataset = tf.data.TextLineDataset(train_flag_file)\n",
    "# Convert string to a float..\n",
    "flag_dataset = flag_dataset.map(lambda sentence: tf.string_to_number(sentence))\n",
    "\n",
    "#Join datasets together, using zip\n",
    "dataset = tf.data.Dataset.zip((context_dataset, response_dataset, flag_dataset))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let us test this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:3 data:(array([    3,    96,   266,   105,  2089,     4,   307,  2708,   338,\n",
      "        1505,     5,    24,    48,    35,   266,    61,     6,   141,\n",
      "         338,  1153,     8,     3,    96,     9,    12,   727,   197,\n",
      "        4965,     4,  1390,    19,   597,   431,    13,   197,  2089,\n",
      "        2118,   115,    13,    35,     1,     2,     9,    81,    38,\n",
      "         230,   416,     6,   414,    10,  3615,  2923,    20,     0,\n",
      "           1,   444,   134,     1,     2,    93,     1,   567,   266,\n",
      "          21,  3458,    56,  5523,     0,     1,     2,   152,     7,\n",
      "           1,     2,   108,     1,     2,    27,   344,     1,     9,\n",
      "          12,  5480,   119,    49, 18947,     7,     1,     2,    80,\n",
      "           1,    24,     9,    30,   159,    27,     4,   109,   142,\n",
      "         438,     1,    28,    10,   861,  2615,  9046,    15,   363,\n",
      "        1642,     1,     2,    11,    67,    38,  1506,   155,    49,\n",
      "        2615,  1116,     7,     1,  1775,    10,   402,  1065,   102,\n",
      "        2952,    23,     6,    16,    32,     4, 16527,    45,    27,\n",
      "          16,    23,    11,  1411,     1,     2,    31,  4935,    12,\n",
      "           0,     7,     1,     2,    27,  3455,  4935,     5,    24,\n",
      "          10,  1814, 12791,     1,     2,     3,  1513,    14,   266,\n",
      "          79,   656,     4,     0,     1,     2,    93,     1,     2,\n",
      "          81,    11,  1733,    10,   124,  2274,    10,  1142,     7,\n",
      "           1,     2,  2834,     7,     1,     2,  9047,     0,    95,\n",
      "          20,     0,     8,    65,  1891,    18,     4,  1247,    20,\n",
      "           4,  1468,    13,    14,    30,     9,     1,     2,  9047,\n",
      "           4,   300,   124,    11,   140,     7,     1,     2,    80,\n",
      "           8,   137,   934,   206,     8,     1,     2,     3,    81,\n",
      "        1102,     6,  1624,     9,    75,    42,  2546,     8,    42,\n",
      "          12,   114,    14,    97,   935,   155,     0,  2209,     5,\n",
      "           3,   947,     7,     1,     2]), array([ 811,  597, 3527, 1891,   67,   27,  922,  419,    6,  229, 5537,\n",
      "         20,  959,   18,  238,    1,   46,  114,    3,   66,   15,   26,\n",
      "       2873,  142,    8,    1]), 1.0)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    data = sess.run(iterator.get_next())\n",
    "    print('Len:{} data:{}'.format(len(data), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you see that data is a tuple of length 3. \n",
    "\n",
    "* `data[0]` is `context`\n",
    "* `data[1]` is `response`\n",
    "* `data[2]` is `flag`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, context is sometimes really long! We will take a cue from the [original paper](http://www.cs.toronto.edu/~lcharlin/papers/ubuntu_dialogue_dd17.pdf), and restrict context to 160 tokens. \n",
    "\n",
    "Further, we will restrict it to only take last 160 tokens. Why? Because the most relevant bits to predict next response would be in the end!\n",
    "\n",
    "We now need to worry about two other things: \n",
    "* Add length of context and response. We will use `tf.size` on the words\n",
    "* Batch data\n",
    "\n",
    "Let us see how this can be accomplished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "#Create context dataset, sentence -> word indexes\n",
    "context_dataset = text_to_word_indexes(train_context_file, vocab_table)\n",
    "\n",
    "#Restrict context to Last 160 tokens\n",
    "context_dataset = context_dataset.map(lambda words: words[-160:])\n",
    "\n",
    "\n",
    "#Create response dataset, sentence -> word indexes\n",
    "response_dataset = text_to_word_indexes(train_response_file, vocab_table)\n",
    "\n",
    "flag_dataset = tf.data.TextLineDataset(train_flag_file)\n",
    "# Convert string to a float..\n",
    "flag_dataset = flag_dataset.map(lambda sentence: tf.string_to_number(sentence))\n",
    "\n",
    "#Join datasets together, using zip\n",
    "dataset = tf.data.Dataset.zip((context_dataset, response_dataset, flag_dataset))\n",
    "\n",
    "#Add length of context and response\n",
    "dataset = dataset.map(lambda context, response, flag: (context, tf.size(context), response, tf.size(response), flag))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:5 data:(array([    2,    80,     1,    24,     9,    30,   159,    27,     4,\n",
      "         109,   142,   438,     1,    28,    10,   861,  2615,  9046,\n",
      "          15,   363,  1642,     1,     2,    11,    67,    38,  1506,\n",
      "         155,    49,  2615,  1116,     7,     1,  1775,    10,   402,\n",
      "        1065,   102,  2952,    23,     6,    16,    32,     4, 16527,\n",
      "          45,    27,    16,    23,    11,  1411,     1,     2,    31,\n",
      "        4935,    12,     0,     7,     1,     2,    27,  3455,  4935,\n",
      "           5,    24,    10,  1814, 12791,     1,     2,     3,  1513,\n",
      "          14,   266,    79,   656,     4,     0,     1,     2,    93,\n",
      "           1,     2,    81,    11,  1733,    10,   124,  2274,    10,\n",
      "        1142,     7,     1,     2,  2834,     7,     1,     2,  9047,\n",
      "           0,    95,    20,     0,     8,    65,  1891,    18,     4,\n",
      "        1247,    20,     4,  1468,    13,    14,    30,     9,     1,\n",
      "           2,  9047,     4,   300,   124,    11,   140,     7,     1,\n",
      "           2,    80,     8,   137,   934,   206,     8,     1,     2,\n",
      "           3,    81,  1102,     6,  1624,     9,    75,    42,  2546,\n",
      "           8,    42,    12,   114,    14,    97,   935,   155,     0,\n",
      "        2209,     5,     3,   947,     7,     1,     2]), 160, array([ 811,  597, 3527, 1891,   67,   27,  922,  419,    6,  229, 5537,\n",
      "         20,  959,   18,  238,    1,   46,  114,    3,   66,   15,   26,\n",
      "       2873,  142,    8,    1]), 26, 1.0)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    data = sess.run(iterator.get_next())\n",
    "    print('Len:{} data:{}'.format(len(data), data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, the tuple length is now 5 (we have added len_context and len_response). But this seems to be getting a bit out of control.\n",
    "\n",
    "We should not be expected to remember which field in tuple means what! Well, we can name these fields using [`namedtuple`](https://docs.python.org/2/library/collections.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "#Notice a new (sixth) field init. This is initializer for iterator\n",
    "class DataIterator(namedtuple('DataIterator', 'init context len_context response len_response flag')):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do return a new `DataIterator` instead of iterator. This would allow us access via named fields!\n",
    "\n",
    "Let us see how! We will put our code from before inside a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_iterator(vocab_table, context_file, response_file, flag_file):\n",
    "    #Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "    vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "    #Create context dataset, sentence -> word indexes\n",
    "    context_dataset = text_to_word_indexes(context_file, vocab_table)\n",
    "\n",
    "    #Restrict context to Last 160 tokens\n",
    "    context_dataset = context_dataset.map(lambda words: words[-160:])\n",
    "\n",
    "\n",
    "    #Create response dataset, sentence -> word indexes\n",
    "    response_dataset = text_to_word_indexes(response_file, vocab_table)\n",
    "\n",
    "    flag_dataset = tf.data.TextLineDataset(flag_file)\n",
    "    # Convert string to a float..\n",
    "    flag_dataset = flag_dataset.map(lambda sentence: tf.string_to_number(sentence))\n",
    "\n",
    "    #Join datasets together, using zip\n",
    "    dataset = tf.data.Dataset.zip((context_dataset, response_dataset, flag_dataset))\n",
    "\n",
    "    #Add length of context and response\n",
    "    dataset = dataset.map(lambda context, response, flag: (context, tf.size(context), response, tf.size(response), flag))\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    context, len_context, response, len_response, flag = iterator.get_next()\n",
    "\n",
    "    return DataIterator(iterator.initializer, context, len_context, response, len_response, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = create_dataset_iterator(vocab_table, train_context_file, train_response_file, train_flag_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:[    2    80     1    24     9    30   159    27     4   109   142   438\n",
      "     1    28    10   861  2615  9046    15   363  1642     1     2    11\n",
      "    67    38  1506   155    49  2615  1116     7     1  1775    10   402\n",
      "  1065   102  2952    23     6    16    32     4 16527    45    27    16\n",
      "    23    11  1411     1     2    31  4935    12     0     7     1     2\n",
      "    27  3455  4935     5    24    10  1814 12791     1     2     3  1513\n",
      "    14   266    79   656     4     0     1     2    93     1     2    81\n",
      "    11  1733    10   124  2274    10  1142     7     1     2  2834     7\n",
      "     1     2  9047     0    95    20     0     8    65  1891    18     4\n",
      "  1247    20     4  1468    13    14    30     9     1     2  9047     4\n",
      "   300   124    11   140     7     1     2    80     8   137   934   206\n",
      "     8     1     2     3    81  1102     6  1624     9    75    42  2546\n",
      "     8    42    12   114    14    97   935   155     0  2209     5     3\n",
      "   947     7     1     2] Len:160\n",
      "Context:[ 811  597 3527 1891   67   27  922  419    6  229 5537   20  959   18  238\n",
      "    1   46  114    3   66   15   26 2873  142    8    1] Len:26\n",
      "Flag: 1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(iterator.init)\n",
    "    \n",
    "    datum = sess.run(iterator)\n",
    "    print ('Context:{} Len:{}'.format(datum.context, datum.len_context))\n",
    "    print ('Context:{} Len:{}'.format(datum.response, datum.len_response))\n",
    "    print ('Flag: {}'.format(datum.flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now to the final part. Batching\n",
    "\n",
    "Batching with text is a bit tricky, as the length of each sentence! Tensorflow supports concept of [`padded_batch`]\n",
    "(https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset#padded_batch). Let us see how to use it!\n",
    "\n",
    "```python\n",
    "dataset = dataset.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([])))\n",
    "```\n",
    "\n",
    "The first argument `batch_size` is exactly what it means: How many lines do you want to pool together?\n",
    "Remaining items specify how padding should be done. For all practical purposes, yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_iterator(vocab_table, context_file, response_file, flag_file, batch_size):\n",
    "    #Create a vocab table. word -> index. Tell it if word is not found, use index 0 `UNK`\n",
    "    vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "    #Create context dataset, sentence -> word indexes\n",
    "    context_dataset = text_to_word_indexes(context_file, vocab_table)\n",
    "\n",
    "    #Restrict context to Last 160 tokens\n",
    "    context_dataset = context_dataset.map(lambda words: words[-160:])\n",
    "\n",
    "\n",
    "    #Create response dataset, sentence -> word indexes\n",
    "    response_dataset = text_to_word_indexes(response_file, vocab_table)\n",
    "\n",
    "    flag_dataset = tf.data.TextLineDataset(flag_file)\n",
    "    # Convert string to a float..\n",
    "    flag_dataset = flag_dataset.map(lambda sentence: tf.string_to_number(sentence))\n",
    "\n",
    "    #Join datasets together, using zip\n",
    "    dataset = tf.data.Dataset.zip((context_dataset, response_dataset, flag_dataset))\n",
    "\n",
    "    #Add length of context and response\n",
    "    dataset = dataset.map(lambda context, response, flag: (context, tf.size(context), response, tf.size(response), flag))\n",
    "\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([None]), tf.TensorShape([]), tf.TensorShape([])))\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "    context, len_context, response, len_response, flag = iterator.get_next()\n",
    "\n",
    "    return DataIterator(iterator.initializer, context, len_context, response, len_response, flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = create_dataset_iterator(vocab_table, train_context_file, train_response_file, train_flag_file, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len datum: 6\n",
      "Context Shape:(16, 160)\n",
      "Len_Context Shape:(16,)\n",
      "Response Shape:(16, 27)\n",
      "Len_Response Shape:(16,)\n",
      "Flags Shape:(16,)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(iterator.init)\n",
    "    \n",
    "    datum = sess.run(iterator)\n",
    "    print('Len datum: {}'.format(len(datum)))\n",
    "    print ('Context Shape:{}'.format(datum.context.shape))\n",
    "    print ('Len_Context Shape:{}'.format(datum.len_context.shape))\n",
    "        \n",
    "    print ('Response Shape:{}'.format(datum.response.shape))\n",
    "    print ('Len_Response Shape:{}'.format(datum.len_response.shape))\n",
    "    \n",
    "    print ('Flags Shape:{}'.format(datum.flag.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is all! We now have a data iterator with named fields, that can return batched data.\n",
    "\n",
    "Well, we are all set to use this data iterator for our Siamase Network for text correlation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
