{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore how to process text data using `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download the english sentences `train.en` and vocab `vocab.en` from [Ted Talks dataset IWSLT'15 English-Vietnamese data.](https://nlp.stanford.edu/projects/nmt/)\n",
    "  ```bash\n",
    "  wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
    "  wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
    "  \n",
    "  head -100 train.en > sample.en\n",
    "  ```\n",
    "  \n",
    "### Install and setup tensorflow\n",
    "https://www.tensorflow.org/install/\n",
    "\n",
    "This notebook requires Python 3 and Tensorflow version >= 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `DATA_DIR` with folder where you downloaded the data. You can replace `sample.en` with `train.en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.'\n",
    "sentences_file = os.path.join(DATA_DIR, 'sample.en')\n",
    "vocab_file = os.path.join(DATA_DIR, 'vocab.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates dataset, and further creates an iterator. Note this can only be **executed** inside a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup table, converts a token to integer. By default returns token at first line of `vocab.en`\n",
    "#Requires to be initialized using tf.tables_initializer inside a session.\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "#Creates a dataset which retruns a single sentence\n",
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "\n",
    "#Converts each sentence to a list of tokens\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "#Converts list of tokens to list of token integers\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "\n",
    "#Adds length of sentence (number of tokens)\n",
    "dataset = dataset.map(lambda words: (words, tf.size(words)))\n",
    "\n",
    "#Convert to a batch of size 32. Padded batch appends 0 for shorter sentences.\n",
    "dataset = dataset.padded_batch(batch_size=32, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])))\n",
    "\n",
    "# Dataset iterator. Needs to be initialized\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #Iterator needs to be initialized\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    #This is required for our lookup table\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    print ('Num Sentences: %d Shape[txt]: %s Shape[len]: %s'%(len(sentences[0]), sentences[0].shape, sentences[1].shape))\n",
    "    print ('S[%d]:%s Length:%d\\n'%(1, sentences[0][1], sentences[1][1]))\n",
    "    print ('S[%d]:%s Length:%d'%(14, sentences[0][14], sentences[1][14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that shape of tokens part is **32x50** (`batch_size` x maximum tokens in the batch). Shape of length is **32** (one per element of batch). Also sentence 14 has length 30. The remaining portion is padded with 0.\n",
    "\n",
    "What is the maximum sentence length for the next batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    print ('Num Sentences: %d Shape[txt]: %s Shape[len]: %s'%(len(sentences[0]), sentences[0].shape, sentences[1].shape))\n",
    "    print ('S[%d]:%s Length:%d\\n'%(1, sentences[0][1], sentences[1][1]))\n",
    "    print ('S[%d]:%s Length:%d'%(14, sentences[0][14], sentences[1][14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that shape of tokens part is **32x40**. Thus, maximum length for the second batch is 40."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
