{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace `DATA_DIR` with folder where you downloaded the data. You can replace `sample.en` with `train.en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.'\n",
    "sentences_file = os.path.join(DATA_DIR, 'sample.en')\n",
    "vocab_file = os.path.join(DATA_DIR, 'vocab.en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates dataset, and further creates an iterator. Note this can only be **executed** inside a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup table, converts a token to integer. By default returns token at first line of `vocab.en`\n",
    "#Requires to be initialized using tf.tables_initializer inside a session.\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "\n",
    "#Creates a dataset which retruns a single sentence\n",
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "\n",
    "#Converts each sentence to a list of tokens\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "#Converts list of tokens to list of token integers\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "\n",
    "#Adds length of sentence (number of tokens)\n",
    "dataset = dataset.map(lambda words: (words, tf.size(words)))\n",
    "\n",
    "#Convert to a batch of size 32. Padded batch appends 0 for shorter sentences.\n",
    "dataset = dataset.padded_batch(batch_size=32, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])))\n",
    "\n",
    "# Dataset iterator. Needs to be initialized\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Sentences: 32 Shape[txt]: (32, 50) Shape[len]: (32,)\n",
      "S[1]:[11 12 13 14 15 16  3  0 17  8 18 19 20 21 22 23  7 20 24 25 26  9 27 14 28\n",
      " 29 30 31 32 19 33 34 35 31 36  8 37 38 39 20 40 41 42 19 43 26  8 44 45 46] Length:50\n",
      "\n",
      "S[14]:[106  32  19  20 173  47 157 121 174   0  14 147 121 175  46 112 113   8\n",
      " 176 177  45  46 178 179 180 181 182  19 143  46   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] Length:30\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    print ('Num Sentences: %d Shape[txt]: %s Shape[len]: %s'%(len(sentences[0]), sentences[0].shape, sentence[1].shape))\n",
    "    print ('S[%d]:%s Length:%d\\n'%(1, sentences[0][1], sentences[1][1]))\n",
    "    print ('S[%d]:%s Length:%d'%(14, sentences[0][14], sentences[1][14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Sentences: 32 Shape[txt]: (32, 40) Shape[len]: (32,)\n",
      "S[1]:[106  26 268  19  56  79 250 167 162 269 270 271  28 140  46   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0] Length:15\n",
      "\n",
      "S[14]:[106  87  19  56  79 250 167 329 180 282   8 171 330 331  26   8 171 330\n",
      " 172  71 173  46   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0] Length:22\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    print ('Num Sentences: %d Shape[txt]: %s Shape[len]: %s'%(len(sentences[0]), sentences[0].shape, sentence[1].shape))\n",
    "    print ('S[%d]:%s Length:%d\\n'%(1, sentences[0][1], sentences[1][1]))\n",
    "    print ('S[%d]:%s Length:%d'%(14, sentences[0][14], sentences[1][14]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
