{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we would look at how to handle text using `tf.data` APIs in tensorflow.\n",
    "\n",
    "More specifically we will look at how to create dataset iterators, which can:\n",
    "* Return one line per call from a text file\n",
    "* Return a list of tokens per line\n",
    "* Return a list of token **integers** by using a lookup table\n",
    "* Append length of sentence (number of tokens)\n",
    "* Return a **batch** of token integers (Think multiple sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download the english sentences `train.en` and vocab `vocab.en` from [Ted Talks dataset IWSLT'15 English-Vietnamese data.](https://nlp.stanford.edu/projects/nmt/)\n",
    "  ```bash\n",
    "  wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
    "  wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
    "  \n",
    "  head -100 train.en > sample.en\n",
    "  ```\n",
    "  \n",
    "  You can go ahead and work with the entire training data `train.en`. We are using a sample for simplicity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, we are using tensorflow, so..\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is to get path for text files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Data iterator for text\n",
    "\n",
    "We will create a data iterator which returns a single sentence per call.\n",
    "\n",
    "Replace `DATA_DIR` with the actual path of directory where you downloaded the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '.'\n",
    "sentences_file = os.path.join(DATA_DIR, 'sample.en')\n",
    "vocab_file = os.path.join(DATA_DIR, 'vocab.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a dataset which returns a single sentence\n",
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "\n",
    "# Dataset iterator. Needs to be initialized\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we did above was create a dataset which returns one sentence per call. We further created a iterator.\n",
    "\n",
    "Note, we cannot execute this as is. This is an operator that returns tensor. We need to run it inside a tensorflow session. Further, we need to **initialize** out iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Rachel Pike : The science behind a climate headline'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    sentence = sess.run(iterator.get_next())\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good! But, what happens when we run out of data items? In our case past sentence 100. Does it return to begnning of file again? Well, lets check it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:10 :b'It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .'\n",
      "S:20 :b'Because it &apos;s important to the atmospheric system , we go to all lengths to study this thing .'\n",
      "S:30 :b'I recently joined a field campaign in Malaysia . There are others .'\n",
      "S:40 :b'We have to get special flight clearance .'\n",
      "S:50 :b'So you can imagine the scale of the effort .'\n",
      "S:60 :b'I &apos;m going to tell you about that technology .'\n",
      "S:70 :b'We don &apos;t have to inject anything . We don &apos;t need radiation .'\n",
      "S:80 :b'We all know that as we form thoughts , they form deep channels in our minds and in our brains .'\n",
      "S:90 :b'As they do it , in the upper left is a display that &apos;s yoked to their brain activation of their own pain being controlled .'\n",
      "S:100 :b'Where will we take it ?'\n"
     ]
    },
    {
     "ename": "OutOfRangeError",
     "evalue": "End of sequence\n\t [[Node: IteratorGetNext_202 = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext_202', defined at:\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-83edb5bcc7a4>\", line 5, in <module>\n    sentence = sess.run(iterator.get_next())\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 268, in get_next\n    name=name)), self._output_types)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 777, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3076, in create_op\n    op_def=op_def)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1561, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_202 = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_202 = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-83edb5bcc7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnum_sentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[Node: IteratorGetNext_202 = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext_202', defined at:\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-83edb5bcc7a4>\", line 5, in <module>\n    sentence = sess.run(iterator.get_next())\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 268, in get_next\n    name=name)), self._output_types)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 777, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3076, in create_op\n    op_def=op_def)\n  File \"/Users/vineet/anaconda/envs/tfn/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1561, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[Node: IteratorGetNext_202 = IteratorGetNext[output_shapes=[[]], output_types=[DT_STRING], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    while True:\n",
    "        sentence = sess.run(iterator.get_next())\n",
    "        num_sentences += 1\n",
    "        \n",
    "        #Print progress every 10 sentences\n",
    "        if num_sentences % 10 == 0:\n",
    "            print('S:%d :%s'%(num_sentences, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a `tf.errors.OutOfRangeError`. This indicates we have run out of things to process. It is **not really an error**. Okay, then, why does this throw an error?\n",
    "\n",
    "Think of `tf.errors.OutOfRangeError` as a signal that you have finished an epoch. You can re-initialize the iterator here. This can be useful, if you want to say do an evaluation at the end of an epoch. The code below stops after two epochs. Notice, how we need to call `sess.run(iterator.initializer)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:10 :b'It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .'\n",
      "S:20 :b'Because it &apos;s important to the atmospheric system , we go to all lengths to study this thing .'\n",
      "S:30 :b'I recently joined a field campaign in Malaysia . There are others .'\n",
      "S:40 :b'We have to get special flight clearance .'\n",
      "S:50 :b'So you can imagine the scale of the effort .'\n",
      "S:60 :b'I &apos;m going to tell you about that technology .'\n",
      "S:70 :b'We don &apos;t have to inject anything . We don &apos;t need radiation .'\n",
      "S:80 :b'We all know that as we form thoughts , they form deep channels in our minds and in our brains .'\n",
      "S:90 :b'As they do it , in the upper left is a display that &apos;s yoked to their brain activation of their own pain being controlled .'\n",
      "S:100 :b'Where will we take it ?'\n",
      "End of epoch: 0 \n",
      "S:110 :b'It &apos;s a big community . It &apos;s such a big community , in fact , that our annual gathering is the largest scientific meeting in the world .'\n",
      "S:120 :b'Because it &apos;s important to the atmospheric system , we go to all lengths to study this thing .'\n",
      "S:130 :b'I recently joined a field campaign in Malaysia . There are others .'\n",
      "S:140 :b'We have to get special flight clearance .'\n",
      "S:150 :b'So you can imagine the scale of the effort .'\n",
      "S:160 :b'I &apos;m going to tell you about that technology .'\n",
      "S:170 :b'We don &apos;t have to inject anything . We don &apos;t need radiation .'\n",
      "S:180 :b'We all know that as we form thoughts , they form deep channels in our minds and in our brains .'\n",
      "S:190 :b'As they do it , in the upper left is a display that &apos;s yoked to their brain activation of their own pain being controlled .'\n",
      "S:200 :b'Where will we take it ?'\n",
      "End of epoch: 1 \n"
     ]
    }
   ],
   "source": [
    "num_sentences = 0\n",
    "epoch_num = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            sentence = sess.run(iterator.get_next())\n",
    "            num_sentences += 1\n",
    "            if num_sentences % 10 == 0:\n",
    "                print('S:%d :%s'%(num_sentences, sentence))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('End of epoch: %d '%epoch_num)\n",
    "            epoch_num += 1\n",
    "            \n",
    "            #Stop after 2 epochs\n",
    "            if epoch_num == 2:\n",
    "                break\n",
    "            sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Return a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We saw this before, just returns one line from text file\n",
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "\n",
    "#This would convert a sentence to a list of tokens.\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "#Good, ol iterator!\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'Rachel' b'Pike' b':' b'The' b'science' b'behind' b'a' b'climate'\n",
      " b'headline']\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    sentence = sess.run(iterator.get_next())\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, how this returns a list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Lookup Table, and list of token *integers*\n",
    "OK, so we have made some progress. We know how to create an iterator, and how to return a list of tokens.\n",
    "\n",
    "To convert tokens (strings) to integers, we need a lookup table. Think of lookup table as a dictionary, which returns a unique index for each token (And a default index for out of vocabulary words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a lookup op, which will convert a string to integer, as per vocab_file. \n",
    "# By default this returns 0 (Look at what is first line of vocab_file)\n",
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oh comeon, we have seen these two lines enough!\n",
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "\n",
    "# This does a lookup. Returns corresponding integers\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "\n",
    "#Ah, I know this!\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  0  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    sentence = sess.run(iterator.get_next())\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Append length of sentence (aka number of tokens)\n",
    "\n",
    "Why would we need to append number of tokens. Well, text is interesting. Not all sentences are of same length. Thus if we are considering batching them together, we need to know where sentence ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "\n",
    "#We can compute length at runtime using tf.size\n",
    "dataset = dataset.map(lambda words: (words, tf.size(words)))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 3,  0,  4,  5,  6,  7,  8,  9, 10]), 9)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    sentence = sess.run(iterator.get_next())\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Step: Batching\n",
    "\n",
    "This step involves grouping sentences together. This is where `tf.data` comes really handy, as it takes care of batching for you. However, text comes with its own set of challenges. \n",
    "\n",
    "Remember, each sentence is not of same length. Solution is to used `padded_batch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(sentences_file)\n",
    "dataset = dataset.map(lambda sentence: tf.string_split([sentence]).values)\n",
    "dataset = dataset.map(lambda words: vocab_table.lookup(words))\n",
    "\n",
    "\n",
    "dataset = dataset.map(lambda words: (words, tf.size(words)))\n",
    "\n",
    "#First argument tells us what is size of batch. [None] says this is vectors of unknown size\n",
    "dataset = dataset.padded_batch(32, padded_shapes=(tf.TensorShape([None]), tf.TensorShape([])))\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    sess.run(tf.tables_initializer())\n",
    "    \n",
    "    #This is now a tuple. Where [0] is sentence [1] is length\n",
    "    sentences = sess.run(iterator.get_next())\n",
    "    print(len(sentences[0]), len(sentences[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
